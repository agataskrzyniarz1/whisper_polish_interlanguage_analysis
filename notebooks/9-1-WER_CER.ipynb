{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b2ddb4-61f8-493a-9848-a9057a4397d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from jiwer import wer, cer\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca0476-5902-434c-b337-97bf7b5fa813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "with open(\"transcriptions_ordonnées.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72a723e4-88aa-4b15-b4a8-456fd6a1924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Statistiques globales par pays ===\n",
      "\n",
      "Pays : FR\n",
      "WER moyen (Auto vs Manuel)  : 0.728\n",
      "WER moyen (Auto vs Correct) : 0.743\n",
      "CER moyen (Auto vs Manuel)  : 0.424\n",
      "CER moyen (Auto vs Correct) : 0.462\n",
      "WER médian (Auto vs Manuel)  : 0.500\n",
      "WER médian (Auto vs Correct) : 0.535\n",
      "CER médian (Auto vs Manuel)  : 0.263\n",
      "CER médian (Auto vs Correct) : 0.286\n",
      "\n",
      "Pays : GE\n",
      "WER moyen (Auto vs Manuel)  : 0.652\n",
      "WER moyen (Auto vs Correct) : 0.755\n",
      "CER moyen (Auto vs Manuel)  : 0.384\n",
      "CER moyen (Auto vs Correct) : 0.435\n",
      "WER médian (Auto vs Manuel)  : 0.406\n",
      "WER médian (Auto vs Correct) : 0.463\n",
      "CER médian (Auto vs Manuel)  : 0.163\n",
      "CER médian (Auto vs Correct) : 0.188\n",
      "\n",
      "Pays : IT\n",
      "WER moyen (Auto vs Manuel)  : 0.917\n",
      "WER moyen (Auto vs Correct) : 0.896\n",
      "CER moyen (Auto vs Manuel)  : 0.615\n",
      "CER moyen (Auto vs Correct) : 0.622\n",
      "WER médian (Auto vs Manuel)  : 0.522\n",
      "WER médian (Auto vs Correct) : 0.475\n",
      "CER médian (Auto vs Manuel)  : 0.238\n",
      "CER médian (Auto vs Correct) : 0.262\n",
      "\n",
      "Pays : NL\n",
      "WER moyen (Auto vs Manuel)  : 0.486\n",
      "WER moyen (Auto vs Correct) : 0.479\n",
      "CER moyen (Auto vs Manuel)  : 0.270\n",
      "CER moyen (Auto vs Correct) : 0.266\n",
      "WER médian (Auto vs Manuel)  : 0.485\n",
      "WER médian (Auto vs Correct) : 0.425\n",
      "CER médian (Auto vs Manuel)  : 0.247\n",
      "CER médian (Auto vs Correct) : 0.200\n",
      "\n",
      "Pays : UK\n",
      "WER moyen (Auto vs Manuel)  : 1.018\n",
      "WER moyen (Auto vs Correct) : 1.036\n",
      "CER moyen (Auto vs Manuel)  : 0.652\n",
      "CER moyen (Auto vs Correct) : 0.677\n",
      "WER médian (Auto vs Manuel)  : 0.852\n",
      "WER médian (Auto vs Correct) : 0.828\n",
      "CER médian (Auto vs Manuel)  : 0.331\n",
      "CER médian (Auto vs Correct) : 0.416\n",
      "\n",
      "=== Statistiques globales ===\n",
      "WER moyen (Auto vs Manuel)  : 0.754\n",
      "WER moyen (Auto vs Correct) : 0.777\n",
      "CER moyen (Auto vs Manuel)  : 0.464\n",
      "CER moyen (Auto vs Correct) : 0.488\n",
      "WER médian (Auto vs Manuel)  : 0.500\n",
      "WER médian (Auto vs Correct) : 0.535\n",
      "CER médian (Auto vs Manuel)  : 0.222\n",
      "CER médian (Auto vs Correct) : 0.262\n"
     ]
    }
   ],
   "source": [
    "# Fonctions utilitaires\n",
    "def concat_segments(d, prefix):\n",
    "    textes = []\n",
    "    i = 1\n",
    "    while f\"{prefix} ({i})\" in d:\n",
    "        textes.append(d[f\"{prefix} ({i})\"])\n",
    "        i += 1\n",
    "    return \" \".join(textes)\n",
    "\n",
    "def clean(t):\n",
    "    return re.sub(r\"[^\\w\\sąćęłńóśźżĄĆĘŁŃÓŚŹŻ-]\", \"\", t.lower().strip())\n",
    "\n",
    "# Initialisation des listes\n",
    "wer_auto_vs_man, wer_auto_vs_corr = [], []\n",
    "cer_auto_vs_man, cer_auto_vs_corr = [], []\n",
    "ids = []\n",
    "\n",
    "#Traitement des exemples\n",
    "for exemple in data:\n",
    "    apprenant_id = exemple.get(\"numéro d'apprenant\", \"inconnu\")\n",
    "    ids.append(apprenant_id)\n",
    "\n",
    "    texte_auto = exemple.get(\"transcription automatique\", \n",
    "                             exemple.get(\"transcription_automatique_non_pl\", \"\"))\n",
    "    texte_man = concat_segments(exemple, \"transcription manuelle\")\n",
    "    texte_corr = concat_segments(exemple, \"version correcte en polonais\")\n",
    "\n",
    "    texte_auto = clean(texte_auto)\n",
    "    texte_man = clean(texte_man)\n",
    "    texte_corr = clean(texte_corr)\n",
    "\n",
    "    wer_auto_vs_man.append(wer(texte_man, texte_auto))\n",
    "    wer_auto_vs_corr.append(wer(texte_corr, texte_auto))\n",
    "    cer_auto_vs_man.append(cer(texte_man, texte_auto))\n",
    "    cer_auto_vs_corr.append(cer(texte_corr, texte_auto))\n",
    "\n",
    "# Dictionnaires pour stocker les scores par pays\n",
    "scores_par_pays = defaultdict(lambda: {\n",
    "    \"wer_man\": [],\n",
    "    \"wer_corr\": [],\n",
    "    \"cer_man\": [],\n",
    "    \"cer_corr\": []\n",
    "})\n",
    "\n",
    "# Remplir les scores\n",
    "for i, exemple in enumerate(data):\n",
    "    pays = exemple.get(\"pays\", \"INCONNU\")\n",
    "    scores_par_pays[pays][\"wer_man\"].append(wer_auto_vs_man[i])\n",
    "    scores_par_pays[pays][\"wer_corr\"].append(wer_auto_vs_corr[i])\n",
    "    scores_par_pays[pays][\"cer_man\"].append(cer_auto_vs_man[i])\n",
    "    scores_par_pays[pays][\"cer_corr\"].append(cer_auto_vs_corr[i])\n",
    "\n",
    "# Affichage par pays\n",
    "print(\"\\n=== Statistiques globales par pays ===\")\n",
    "for pays in sorted(scores_par_pays.keys()):\n",
    "    s = scores_par_pays[pays]\n",
    "    print(f\"\\nPays : {pays}\")\n",
    "    print(f\"WER moyen (Auto vs Manuel)  : {np.mean(s['wer_man']):.3f}\")\n",
    "    print(f\"WER moyen (Auto vs Correct) : {np.mean(s['wer_corr']):.3f}\")\n",
    "    print(f\"CER moyen (Auto vs Manuel)  : {np.mean(s['cer_man']):.3f}\")\n",
    "    print(f\"CER moyen (Auto vs Correct) : {np.mean(s['cer_corr']):.3f}\")\n",
    "    print(f\"WER médian (Auto vs Manuel)  : {np.median(s['wer_man']):.3f}\")\n",
    "    print(f\"WER médian (Auto vs Correct) : {np.median(s['wer_corr']):.3f}\")\n",
    "    print(f\"CER médian (Auto vs Manuel)  : {np.median(s['cer_man']):.3f}\")\n",
    "    print(f\"CER médian (Auto vs Correct) : {np.median(s['cer_corr']):.3f}\")\n",
    "\n",
    "\n",
    "# Moyennes et médianes globales \n",
    "print(\"\\n=== Statistiques globales ===\")\n",
    "print(f\"WER moyen (Auto vs Manuel)  : {np.mean(wer_auto_vs_man):.3f}\")\n",
    "print(f\"WER moyen (Auto vs Correct) : {np.mean(wer_auto_vs_corr):.3f}\")\n",
    "print(f\"CER moyen (Auto vs Manuel)  : {np.mean(cer_auto_vs_man):.3f}\")\n",
    "print(f\"CER moyen (Auto vs Correct) : {np.mean(cer_auto_vs_corr):.3f}\")\n",
    "\n",
    "print(f\"WER médian (Auto vs Manuel)  : {np.median(wer_auto_vs_man):.3f}\")\n",
    "print(f\"WER médian (Auto vs Correct) : {np.median(wer_auto_vs_corr):.3f}\")\n",
    "print(f\"CER médian (Auto vs Manuel)  : {np.median(cer_auto_vs_man):.3f}\")\n",
    "print(f\"CER médian (Auto vs Correct) : {np.median(cer_auto_vs_corr):.3f}\")\n",
    "\n",
    "# WER & CER auto vs manuelle\n",
    "plt.figure(figsize=(30, 6))\n",
    "plt.plot(ids, wer_auto_vs_man, marker='o', label=\"WER\", color='blue')\n",
    "plt.plot(ids, cer_auto_vs_man, marker='x', label=\"CER\", color='orange')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylim(0, 4)\n",
    "plt.title(\"Auto vs Manuel : WER & CER\")\n",
    "plt.xlabel(\"ID Apprenant\")\n",
    "plt.ylabel(\"Taux d'erreur\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/wer_cer_auto_vs_man.png\")\n",
    "plt.close()\n",
    "\n",
    "# WER & CER auto vs correct\n",
    "plt.figure(figsize=(30, 6))\n",
    "plt.plot(ids, wer_auto_vs_corr, marker='o', label=\"WER\", color='green')\n",
    "plt.plot(ids, cer_auto_vs_corr, marker='x', label=\"CER\", color='red')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylim(0, 4)\n",
    "plt.title(\"Auto vs Correct : WER & CER\")\n",
    "plt.xlabel(\"ID Apprenant\")\n",
    "plt.ylabel(\"Taux d'erreur\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/wer_cer_auto_vs_corr.png\")\n",
    "plt.close()\n",
    "\n",
    "# Comparaison globale des 4 courbes\n",
    "plt.figure(figsize=(30, 6))\n",
    "plt.plot(ids, wer_auto_vs_man, marker='o', label=\"WER Auto vs Man\", color='blue')\n",
    "plt.plot(ids, cer_auto_vs_man, marker='x', label=\"CER Auto vs Man\", color='orange')\n",
    "plt.plot(ids, wer_auto_vs_corr, marker='o', label=\"WER Auto vs Corr\", color='green')\n",
    "plt.plot(ids, cer_auto_vs_corr, marker='x', label=\"CER Auto vs Corr\", color='red')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylim(0, 4)\n",
    "plt.title(\"Comparaison globale : WER & CER\")\n",
    "plt.xlabel(\"ID Apprenant\")\n",
    "plt.ylabel(\"Taux d'erreur\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/wer_cer_comparison.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f22db7-53f8-4e0a-8aa7-fc749c07b78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Résultats par exemple ===\n",
      "\n",
      "Monika\n",
      "WER : 0.075\n",
      "CER : 0.043\n",
      "\n",
      "Ola\n",
      "WER : 0.180\n",
      "CER : 0.064\n",
      "\n",
      "Magda\n",
      "WER : 0.167\n",
      "CER : 0.081\n",
      "\n",
      "Klaudia\n",
      "WER : 0.139\n",
      "CER : 0.072\n",
      "\n",
      "=== Résultats globaux ===\n",
      "WER global : 0.137\n",
      "CER global : 0.064\n"
     ]
    }
   ],
   "source": [
    "# Corpus polonais\n",
    "\n",
    "# Chargement des données depuis le fichier JSON\n",
    "with open(\"corpus_polonais_natif.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Listes pour le calcul global\n",
    "references = []\n",
    "hypotheses = []\n",
    "\n",
    "print(\"=== Résultats par exemple ===\\n\")\n",
    "\n",
    "for exemple in data:\n",
    "    ref = exemple[\"transcription manuelle\"]\n",
    "    hyp = exemple[\"transcription automatique\"]\n",
    "\n",
    "    # Calcul du WER et du CER\n",
    "    wer_val = wer(ref, hyp)\n",
    "    cer_val = cer(ref, hyp)\n",
    "\n",
    "    print(f\"{exemple['name']}\")\n",
    "    print(f\"WER : {wer_val:.3f}\")\n",
    "    print(f\"CER : {cer_val:.3f}\\n\")\n",
    "\n",
    "    references.append(ref)\n",
    "    hypotheses.append(hyp)\n",
    "\n",
    "# Calcul global (toutes les transcriptions concaténées)\n",
    "ref_global = \" \".join(references)\n",
    "hyp_global = \" \".join(hypotheses)\n",
    "\n",
    "wer_global = wer(ref_global, hyp_global)\n",
    "cer_global = cer(ref_global, hyp_global)\n",
    "\n",
    "print(\"=== Résultats globaux ===\")\n",
    "print(f\"WER global : {wer_global:.3f}\")\n",
    "print(f\"CER global : {cer_global:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f80850-3252-4968-802b-0612e4d6103f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
