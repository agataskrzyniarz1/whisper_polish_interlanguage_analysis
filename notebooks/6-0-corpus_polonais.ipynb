{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67d165c7-56ec-4c3f-b80c-0398f82a8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65b0e048-2e74-4b02-bf3a-829663354de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins vers les dossiers\n",
    "BASE_DIR = \"data/PL\"\n",
    "MANUELLES_DIR = os.path.join(BASE_DIR, \"manuelles\")\n",
    "AUTOMATIQUES_DIR = os.path.join(BASE_DIR, \"automatiques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e4c4796-d921-48a1-8136-9ef0c17057c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'adama', 'budynek', 'być', 'będzie', 'cały', 'całą', 'chciał', 'curie', 'czas', 'cztery', 'czwarty', 'dalej', 'dobra', 'dobrze', 'dobrą', 'dojść', 'dokładnie', 'domek', 'gdzie', 'i', 'idzie', 'iść', 'jest', 'juliusza', 'już', 'ją', 'kierować', 'kolejowej', 'lewej', 'lewo', 'marii', 'mickiewicza', 'mija', 'minąć', 'międzyczasie', 'musi', 'na', 'nadal', 'najbliższe', 'najpierw', 'należy', 'następnie', 'niską', 'nią', 'no', 'numer', 'pan', 'pasy', 'pewno', 'pierwszym', 'po', 'potem', 'powinien', 'powinieneś', 'prawo', 'prosto', 'proszę', 'przejść', 'przez', 'przy', 'pójść', 'rozpozna', 'się', 'sklep', 'sklepem', 'skrzyżowanie', 'skrzyżowaniu', 'skręca', 'skręcić', 'skłodowskiej', 'stacji', 'stronie', 'szpitalu', 'szybko', 'słowackiego', 'tak', 'taki', 'tam', 'teatr', 'tej', 'teraz', 'też', 'to', 'trzeba', 'ulica', 'ulicy', 'ulicą', 'ulicę', 'w', 'wyjść', 'z', 'za', 'zaraz', 'ze', 'zielony']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensemble pour stocker les mots uniques\n",
    "mots_uniques = set()\n",
    "\n",
    "# Parcourir tous les fichiers .eaf dans le dossier\n",
    "for nom_fichier in os.listdir(MANUELLES_DIR):\n",
    "    if nom_fichier.endswith(\".eaf\"):\n",
    "        chemin_fichier = os.path.join(MANUELLES_DIR, nom_fichier)\n",
    "        arbre = ET.parse(chemin_fichier)\n",
    "        racine = arbre.getroot()\n",
    "\n",
    "        # Trouver le tier avec l'ID \"%pol\"\n",
    "        for tier in racine.findall(\".//TIER[@TIER_ID='%pol']\"):\n",
    "            for annotation in tier.findall(\".//ALIGNABLE_ANNOTATION/ANNOTATION_VALUE\"):\n",
    "                texte = annotation.text.strip().lower()  # Nettoyer et mettre en minuscules\n",
    "                mots = texte.split()  # Diviser le texte en mots (selon les espaces)\n",
    "                mots_uniques.update(mots)  # Ajouter les mots à l'ensemble (pas de doublons)\n",
    "\n",
    "# Convertir l'ensemble en liste et trier alphabétiquement\n",
    "liste_mots = sorted(mots_uniques)\n",
    "\n",
    "# Afficher la liste des mots uniques\n",
    "print(liste_mots)\n",
    "len(liste_mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5af43c6-4f8c-46f0-a76c-22db4d8bb095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle spaCy pour le polonais\n",
    "nlp = spacy.load(\"pl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a37e71d-356d-495c-9da9-aee91cd28111",
   "metadata": {},
   "outputs": [],
   "source": [
    "donnees = []\n",
    "\n",
    "for mot in liste_mots:\n",
    "    doc = nlp(mot)\n",
    "    for token in doc:\n",
    "        # Garder seulement les noms (NOUN) et adjectifs (ADJ)\n",
    "        if token.pos_ in [\"NOUN\", \"ADJ\"]:\n",
    "            # Récupérer le cas grammatical si disponible\n",
    "            cas = token.morph.get(\"Case\")\n",
    "            donnees.append({\n",
    "                \"mot\": token.text,\n",
    "                \"cas\": cas[0] if cas else \"-\"\n",
    "            })\n",
    "\n",
    "# Créer un tableau avec pandas\n",
    "df_spacy = pd.DataFrame(donnees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78a1efe4-6681-49e5-afaa-48daf2463e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder dans un fichier CSV\n",
    "df_spacy.to_csv(\"corpus_polonais_cas_spacy.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5998bbd-fd98-41b4-806e-2546798ebd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it': ['teraz']}\n"
     ]
    }
   ],
   "source": [
    "# Langues cibles à détecter\n",
    "langues_cibles = {\"fr\", \"it\", \"nl\", \"en\", \"de\"}\n",
    "\n",
    "# Dictionnaire pour stocker les mots détectés par langue\n",
    "mots_par_langue = {code: set() for code in langues_cibles}\n",
    "\n",
    "# Détection de la langue pour chaque mot unique\n",
    "for mot in liste_mots:\n",
    "    try:\n",
    "        if len(mot) < 3:\n",
    "            continue  # Ignorer les mots trop courts (peu fiables)\n",
    "        langue = detect(mot)  # Détecter la langue du mot\n",
    "        if langue in langues_cibles:\n",
    "            mots_par_langue[langue].add(mot)  # Ajouter au bon groupe\n",
    "    except LangDetectException:\n",
    "        continue  # Passer les cas non détectables (ex. mot vide)\n",
    "\n",
    "# Convertir les ensembles en listes triées\n",
    "mots_par_langue_final = {\n",
    "    lang: sorted(list(mots)) for lang, mots in mots_par_langue.items() if mots\n",
    "}\n",
    "\n",
    "# Affichage du dictionnaire final (comme demandé)\n",
    "print(mots_par_langue_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "893a7b26-73cb-480f-b302-8912d84a895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dossier corrigé\n",
    "df_corrige = pd.read_csv('corpus_polonais_cas_corrigé.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44c8a232-21d5-4033-b098-d72c72452728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Acc       1.00      0.50      0.67         2\n",
      "         Dat       0.00      0.00      0.00         0\n",
      "         Gen       0.25      0.67      0.36         3\n",
      "         Ins       0.86      1.00      0.92         6\n",
      "         Loc       0.00      0.00      0.00         7\n",
      "         Nom       0.79      0.94      0.86        16\n",
      "         UNK       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.67        36\n",
      "   macro avg       0.41      0.44      0.40        36\n",
      "weighted avg       0.57      0.67      0.60        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comparaison des cas\n",
    "y_pred = df_spacy['cas'].fillna('UNK')\n",
    "y_true = df_corrige['cas'].fillna('UNK')\n",
    "\n",
    "# Rapport de classification\n",
    "report = classification_report(y_true, y_pred, zero_division=0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7a13df1-2f61-4190-9c82-e8449cb157be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision de spaCy sur l'attribution des cas : 66.67%\n"
     ]
    }
   ],
   "source": [
    "# Précision\n",
    "mask_predicted = df_spacy['cas'] != ''\n",
    "true_positives = (df_spacy['cas'] == df_corrige['cas']) & mask_predicted\n",
    "\n",
    "precision_spacy = true_positives.sum() / mask_predicted.sum()\n",
    "\n",
    "print(f\"Précision de spaCy sur l'attribution des cas : {precision_spacy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5698751f-ac20-4e0f-807a-13c61a692bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus enregistré.\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour nettoyer la transcription automatique\n",
    "def clean_text(text):\n",
    "    # supprimer la ponctuation\n",
    "    text = re.sub(r\"[.,!?;:]\", \"\", text)\n",
    "    # mettre tout en minuscules\n",
    "    text = text.lower()\n",
    "    # supprimer les espaces et retours multiples\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Fonction pour extraire la transcription manuelle d’un fichier .eaf\n",
    "def extract_manual_transcription(eaf_file):\n",
    "    tree = ET.parse(eaf_file)\n",
    "    root = tree.getroot()\n",
    "    manual_segments = []\n",
    "    # chercher la tier \"%pol\" et récupérer toutes les annotations\n",
    "    for tier in root.findall(\".//TIER[@TIER_ID='%pol']\"):\n",
    "        for ann in tier.findall(\"ANNOTATION/ALIGNABLE_ANNOTATION/ANNOTATION_VALUE\"):\n",
    "            manual_segments.append(ann.text.strip())\n",
    "    # concaténer les segments en un seul texte\n",
    "    return \" \".join(manual_segments)\n",
    "\n",
    "# Fonction pour extraire et nettoyer la transcription automatique d’un fichier .txt\n",
    "def extract_auto_transcription(txt_file):\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    return clean_text(content)\n",
    "\n",
    "def main():\n",
    "    corpus = []\n",
    "\n",
    "    # parcourir les fichiers .eaf dans le dossier manuelles\n",
    "    for filename in os.listdir(MANUELLES_DIR):\n",
    "        if filename.endswith(\".eaf\"):\n",
    "            name = os.path.splitext(filename)[0]  # ex. \"Klaudia\"\n",
    "            eaf_path = os.path.join(MANUELLES_DIR, filename)\n",
    "            txt_path = os.path.join(AUTOMATIQUES_DIR, f\"{name}.txt\")\n",
    "\n",
    "            # vérifier si le fichier automatique correspondant existe\n",
    "            if not os.path.exists(txt_path):\n",
    "                print(f\"Fichier automatique manquant pour {name}\")\n",
    "                continue\n",
    "\n",
    "            # extraire les deux transcriptions\n",
    "            manual = extract_manual_transcription(eaf_path)\n",
    "            auto = extract_auto_transcription(txt_path)\n",
    "\n",
    "            # ajouter un exemple au corpus\n",
    "            corpus.append({\n",
    "                \"name\": name,\n",
    "                \"transcription manuelle\": manual,\n",
    "                \"transcription automatique\": auto\n",
    "            })\n",
    "\n",
    "    with open(\"corpus_polonais_natif.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(corpus, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Corpus enregistré.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52832b5-925b-45ce-99c6-131b9d571bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
