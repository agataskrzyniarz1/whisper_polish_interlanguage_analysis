{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "76f5d804-c1f6-4afa-80e6-8d189be2ce45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /home/agata/miniconda3/lib/python3.11/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /home/agata/miniconda3/lib/python3.11/site-packages (from langdetect) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66785b53-40d9-408f-b042-263db7481086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import re\n",
    "from langdetect import detect\n",
    "#from langdetect.lang_detect_exception import LangDetectException\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db78696-5edd-4d70-ad87-078e3e710256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des dossiers\n",
    "data_folder = \"data\"\n",
    "folders = [\"FR\", \"IT\", \"NL\", \"UK\", \"GE\"]\n",
    "input_subfolder = \"corrigées\"\n",
    "output_subfolder = \"corrigées_xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71326b5-5698-4314-8ac7-f8fed11e0d91",
   "metadata": {},
   "source": [
    "### Création de fichiers `.xml` structurés pour les transcriptions manuelles (avec les énoncés des apprenants et ses versions correctes en polonais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "273970cc-6e7d-4692-be50-ed387e9aa8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Fonction pour supprimer les caractères '#' (qui marquent des longues pauses)\"\"\"\n",
    "    if text:\n",
    "        return text.replace(\"#\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e96d99bd-95ca-4cc9-993b-102875930e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LANGUES_A_EXCLURE = {\"fr\", \"it\", \"nl\", \"de\", \"en\"}\n",
    "\n",
    "#mots_detectés_global = {lang: set() for lang in LANGUES_A_EXCLURE}\n",
    "\n",
    "#def filter_text(text):\n",
    "#    \"\"\"\n",
    "#    Supprime les mots détectés comme appartenant aux langues suivantes :\n",
    "#    français, italien, anglais, allemand et néerlandais.\n",
    "#    Ajoute les mots détectés à un dictionnaire global par langue.\n",
    "#    \"\"\"\n",
    "#    mots = re.findall(r\"\\w+\", text)\n",
    "#    mots_gardés = []\n",
    "\n",
    "#    for mot in mots:\n",
    "#        try:\n",
    "#            langue = detect(mot.lower())\n",
    "#        except LangDetectException:\n",
    "#            langue = \"unk\"\n",
    "#        if langue in LANGUES_A_EXCLURE:\n",
    "#            mots_detectés_global[langue].add(mot)\n",
    "#        else:\n",
    "#            mots_gardés.append(mot)\n",
    "\n",
    "#    return \" \".join(mots_gardés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cc2d93e-bf49-46d7-9ba2-e4bbdf4657de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_eaf_files(data_folder, folders, input_subfolder=\"corrigées\", output_subfolder=\"corrigées_xml\"):\n",
    "    \"\"\"\n",
    "    Cette fonction parcourt les fichiers .eaf dans les dossiers spécifiés,\n",
    "    extrait les annotations des apprenants (*STU) et les annotations avec la version correcte en polonais (%pol),\n",
    "    puis génère un fichier XML structuré pour chaque fichier .eaf.\n",
    "    \n",
    "    Args:\n",
    "        data_folder (str): Chemin du dossier principal contenant les sous-dossiers.\n",
    "        folders (list): Liste des sous-dossiers correspondant aux langues (FR, IT, NL, UK, GE).\n",
    "        input_subfolder (str): Nom du sous-dossier contenant les fichiers .eaf.\n",
    "        output_subfolder (str): Nom du sous-dossier où stocker les fichiers XML générés.\n",
    "    \"\"\"\n",
    "    for folder in folders:\n",
    "        input_path = os.path.join(data_folder, folder, input_subfolder)\n",
    "        output_path = os.path.join(data_folder, folder, output_subfolder)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        for filename in os.listdir(input_path):\n",
    "            if filename.endswith(\".eaf\"):\n",
    "                input_file = os.path.join(input_path, filename)\n",
    "                output_file = os.path.join(output_path, filename.replace(\".eaf\", \".xml\"))\n",
    "\n",
    "                # Charger le fichier EAF\n",
    "                tree = ET.parse(input_file)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                time_slots = {}\n",
    "                annotations_stu = []\n",
    "                annotations_pol = []\n",
    "                student_id = None\n",
    "\n",
    "                # Extraire les time slots\n",
    "                for time_slot in root.find(\"TIME_ORDER\").findall(\"TIME_SLOT\"):\n",
    "                    time_slots[time_slot.attrib[\"TIME_SLOT_ID\"]] = int(time_slot.attrib[\"TIME_VALUE\"])\n",
    "\n",
    "                # Extraire les annotations\n",
    "                for tier in root.findall(\"TIER\"):\n",
    "                    tier_id = tier.attrib[\"TIER_ID\"]\n",
    "                    for annotation in tier.findall(\"ANNOTATION\"):\n",
    "                        alignable_annotation = annotation.find(\"ALIGNABLE_ANNOTATION\")\n",
    "                        start = time_slots[alignable_annotation.attrib[\"TIME_SLOT_REF1\"]]\n",
    "                        end = time_slots[alignable_annotation.attrib[\"TIME_SLOT_REF2\"]]\n",
    "                        text = alignable_annotation.find(\"ANNOTATION_VALUE\").text\n",
    "                        text = clean_text(text)  # Supprimer les '#'\n",
    "                        if tier_id == \"*STU\":\n",
    "                            if student_id is None:\n",
    "                                student_id = text  # Premier segment *STU = numéro d'apprenant\n",
    "                            else:\n",
    "                                annotations_stu.append((start, end, text))\n",
    "                        elif tier_id == \"%pol\":\n",
    "                            annotations_pol.append((start, end, text))\n",
    "\n",
    "                # Trier par ordre chronologique\n",
    "                annotations_stu.sort()\n",
    "                annotations_pol.sort()\n",
    "\n",
    "                # Créer un nouvel arbre XML\n",
    "                new_root = ET.Element(\"annotations\")\n",
    "\n",
    "                if student_id:\n",
    "                    student_elem = ET.SubElement(new_root, \"apprenant_id\")\n",
    "                    student_elem.text = student_id\n",
    "\n",
    "                    country_elem = ET.SubElement(new_root, \"pays\")\n",
    "                    country_elem.text = folder\n",
    "\n",
    "                for i, (stu, pol) in enumerate(zip(annotations_stu, annotations_pol), start=1):\n",
    "                    pair = ET.SubElement(new_root, \"énoncé\", id=str(i))\n",
    "                    stu_elem = ET.SubElement(pair, \"STU\")\n",
    "                    stu_elem.text = stu[2]\n",
    "                    pol_elem = ET.SubElement(pair, \"POL\")\n",
    "                    pol_elem.text = pol[2]\n",
    "\n",
    "                # Sauvegarder le fichier\n",
    "                new_tree = ET.ElementTree(new_root)\n",
    "                new_tree.write(output_file, encoding=\"UTF-8\", xml_declaration=True)\n",
    "\n",
    "process_eaf_files(data_folder, folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "337b0954-9846-4a88-a170-46b0070aa82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mots_detectés_global = {lang: sorted(list(mots)) for lang, mots in mots_detectés_global.items() if mots}\n",
    "#print(mots_detectés_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821a5c93-9647-4832-8077-0e6c323ff1b0",
   "metadata": {},
   "source": [
    "### Nettoyage des transcriptions automatiques et l'intégration à l'ensemble des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8497a8b-dad5-42ec-a3a8-28b3fc1c004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Fonction pour supprimer la ponctuation (sauf les tirets), convertir toutes les lettres en minuscules, \n",
    "    supprimer les retours à la ligne et enlever les espaces en début/fin.\n",
    "    \"\"\"\n",
    "    # Remplacer les retours à la ligne par un espace\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Supprimer la ponctuation sauf les tirets et convertir toutes les lettres en minuscules\n",
    "    cleaned_text = ''.join([char.lower() if char.isalnum() or char.isspace() or char == '-' else '' for char in text])\n",
    "\n",
    "    # Supprimer les espaces en début et fin\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb6c5ecb-6064-47e6-b2da-a2148d608ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_automatiques_folder(data_folder, folders, input_subfolder=\"automatiques\"):\n",
    "    \"\"\"Traite les transcriptions automatiques et les nettoie.\"\"\"\n",
    "    cleaned_data = []\n",
    "\n",
    "    for folder in folders:\n",
    "        input_path = os.path.join(data_folder, folder, input_subfolder)\n",
    "\n",
    "        if not os.path.exists(input_path):\n",
    "            print(f\"Le dossier {input_path} n'existe pas.\")\n",
    "            continue\n",
    "\n",
    "        for filename in os.listdir(input_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                input_file = os.path.join(input_path, filename)\n",
    "\n",
    "                with open(input_file, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "\n",
    "                try:\n",
    "                    langue_detectee = detect(content)\n",
    "                except Exception:\n",
    "                    langue_detectee = \"unknown\"\n",
    "\n",
    "                cleaned_content = clean_text(content)\n",
    "                file_number = filename.replace(\".txt\", \"\")\n",
    "\n",
    "                if langue_detectee == 'pl':\n",
    "                    cleaned_data.append({\n",
    "                        'dossier': folder,\n",
    "                        'fichier': file_number,\n",
    "                        'transcription automatique': cleaned_content\n",
    "                    })\n",
    "                else:\n",
    "                    cleaned_data.append({\n",
    "                        'dossier': folder,\n",
    "                        'fichier': file_number,\n",
    "                        'transcription_automatique_non_pl': cleaned_content\n",
    "                    })\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42f7004c-1c11-4d68-9e14-dd7a6c225d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_double_spaces(text):\n",
    "    \"\"\"Fonction pour supprimer les espaces doubles.\"\"\"\n",
    "    return ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50fe19a7-3a57-4d67-809a-d3416144a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_cleaned_data(cleaned_data, data_folder, folders, xml_subfolder=\"corrigées_xml\"):\n",
    "    enriched_data = []\n",
    "\n",
    "    for folder in folders:\n",
    "        xml_path = os.path.join(data_folder, folder, xml_subfolder)\n",
    "        xml_data = {}\n",
    "\n",
    "        for xml_file in os.listdir(xml_path):\n",
    "            if xml_file.endswith(\".xml\"):\n",
    "                file_number = xml_file.replace(\".xml\", \"\")\n",
    "                file_path = os.path.join(xml_path, xml_file)\n",
    "\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                apprenant_id = root.find(\"apprenant_id\").text if root.find(\"apprenant_id\") is not None else \"\"\n",
    "                pays = root.find(\"pays\").text if root.find(\"pays\") is not None else \"\"\n",
    "\n",
    "                segment_dict = {}\n",
    "                for i, enonce in enumerate(root.findall(\"énoncé\"), start=1):\n",
    "                    stu_text = enonce.find(\"STU\").text or \"\"\n",
    "                    pol_text = enonce.find(\"POL\").text or \"\"\n",
    "\n",
    "                    stu_text = clean_double_spaces(stu_text)\n",
    "                    pol_text = clean_double_spaces(pol_text)\n",
    "\n",
    "                    segment_dict[f\"transcription manuelle ({i})\"] = stu_text\n",
    "                    segment_dict[f\"version correcte en polonais ({i})\"] = pol_text\n",
    "\n",
    "                xml_data[file_number] = {\n",
    "                    \"apprenant_id\": apprenant_id,\n",
    "                    \"pays\": pays,\n",
    "                    **segment_dict\n",
    "                }\n",
    "\n",
    "        for entry in cleaned_data:\n",
    "            file_number = entry[\"fichier\"]\n",
    "\n",
    "            if file_number in xml_data:\n",
    "                base_info = {\n",
    "                    \"numéro d'apprenant\": xml_data[file_number][\"apprenant_id\"],\n",
    "                    \"pays\": xml_data[file_number][\"pays\"]\n",
    "                }\n",
    "\n",
    "                # Ajouter la bonne transcription\n",
    "                if \"transcription automatique\" in entry:\n",
    "                    base_info[\"transcription automatique\"] = entry[\"transcription automatique\"]\n",
    "                elif \"transcription_automatique_non_pl\" in entry:\n",
    "                    base_info[\"transcription_automatique_non_pl\"] = entry[\"transcription_automatique_non_pl\"]\n",
    "\n",
    "                segments = {k: v for k, v in xml_data[file_number].items() if k not in [\"apprenant_id\", \"pays\"]}\n",
    "                enriched_entry = {**base_info, **segments}\n",
    "                enriched_data.append(enriched_entry)\n",
    "\n",
    "    return enriched_data\n",
    "\n",
    "# Appels des fonctions principales\n",
    "cleaned_data = process_automatiques_folder(data_folder, folders)\n",
    "enriched_data = enrich_cleaned_data(cleaned_data, data_folder, folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0170cc1c-78b6-427b-bf43-dd70c0da5a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du résultat final\n",
    "output_json_path = os.path.join(data_folder, \"../transcriptions_ordonnées.json\")\n",
    "\n",
    "with open(output_json_path, mode=\"w\", encoding=\"utf-8\") as jsonfile:\n",
    "    json.dump(enriched_data, jsonfile, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f88f2ab-3fc7-4acd-b14b-c4840a80224f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcriptions automatiques NON polonaises\n",
      "Apprenant : 1114, Pays : FR\n",
      "Apprenant : 1105, Pays : FR\n",
      "Apprenant : 5114, Pays : IT\n",
      "Apprenant : 5105, Pays : IT\n",
      "Apprenant : 5120, Pays : IT\n",
      "Apprenant : 5109, Pays : IT\n",
      "Apprenant : 3103, Pays : UK\n",
      "Apprenant : 3118, Pays : UK\n",
      "Apprenant : 3101, Pays : UK\n",
      "Apprenant : 3104, Pays : UK\n",
      "Apprenant : 3105, Pays : UK\n",
      "Apprenant : 3106, Pays : UK\n",
      "Apprenant : 4101, Pays : GE\n",
      "Apprenant : 4118, Pays : GE\n",
      "Apprenant : 4108, Pays : GE\n",
      "\n",
      "Nombre total d'exemples non polonais : 15\n"
     ]
    }
   ],
   "source": [
    "# Affichage des exemples dont la transcription automatique n'est pas en polonais\n",
    "non_pl_examples = [\n",
    "    (entry[\"numéro d'apprenant\"], entry[\"pays\"])\n",
    "    for entry in enriched_data\n",
    "    if \"transcription_automatique_non_pl\" in entry\n",
    "]\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Transcriptions automatiques NON polonaises\")\n",
    "for apprenant_id, pays in non_pl_examples:\n",
    "    print(f\"Apprenant : {apprenant_id}, Pays : {pays}\")\n",
    "\n",
    "print(f\"\\nNombre total d'exemples non polonais : {len(non_pl_examples)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b7704-59e6-452a-a408-608c6d9f3e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
